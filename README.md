# Fine-tune LLaMA 2 (7B) with LoRA on meta-math/MetaMathQA
Fine-tuning and Inference codes on the MetaMath Dataset

[![Code License](https://img.shields.io/badge/Code%20License-MIT-green.svg)](CODE_LICENSE)
[![License](https://img.shields.io/badge/Running%20on-GPU-red.svg)](https://github.com/SuperBruceJia/MetaMath-Fine-Tune-with-LoRA)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

|       Epoch       | Accuracy on the testing set | Model Link |
|:---:|:----:|:----:|
| 1 | 0.609 | ðŸ¤— [Hugging Face](https://huggingface.co/shuyuej/metamath_lora_llama2_7b) |
| 2 | 0.635 | ðŸ¤— [Hugging Face](https://huggingface.co/shuyuej/metamath_lora_llama2_7b_2_epoch)  |
| 3 | 0.641 | ðŸ¤— [Hugging Face](https://huggingface.co/shuyuej/metamath_lora_llama2_7b_3_epoch) |
| 4 | 0.641 | ðŸ¤— [Hugging Face](https://huggingface.co/shuyuej/metamath_lora_llama2_7b_4_epoch) |
